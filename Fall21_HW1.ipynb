{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rnIuPR8EPpQ"
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "In this problem, you will practice training and evaluating a machine learning model on a simple dataset with Scikit-Learn. We will use the Decision Tree algorithm to demonstrate a typical training process.\n",
    "\n",
    "Simply put, the idea of training a predictive model is that, given the available data, we want the model to learn the relationship between some underlying patterns within the data and the expected outputs. We hope that after the learning process, the model will be able to reliably predict the outputs of future unseen data.\n",
    "\n",
    "How do we estimate our model's performance on future unseen data even though we do not have access to such data at the moment? If we can safely assume that the future data follows the same distribution as our training data, we can simulate the future scenario by splitting our available data into a training set and a testing set. We train our model on the training set and evaluate it on the testing set, which serves as a proxy for future data because our model never sees the examples from the testing set during training.\n",
    "\n",
    "If we assume the future data follows the same distribution as our current data, why bother splitting? We can just use the model's training performance as the estimate for future performance. This is actually not true. After all, the current data and the future data are just 2 samples of a distribution so there are still differences and variations between them. You may have a \"lucky\" training set that leads to high training performance. Worse, your machine learning model may have memorized the specific details of the current dataset instead of figuring out the general patterns. In that case, we say the model has overfitted the training data. Overfitted model performs poorly on unseen data that it has not \"memorized\" yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfdafgFTCdno"
   },
   "source": [
    "# Homework 1\n",
    "\n",
    "**DUE Monday, OCT 11th by 11:59PM**\n",
    "\n",
    "The homework in this course is distributed and submitted as iPython notebooks. \n",
    "\n",
    "In an iPython notebook, chunks of codes or texts are executed in individual cells. Texts are written in Markdown cell, as you can see in the homework instructions below. Codes are written in code cells. Cells are run one at a time. You can change the cell type by navigating to Cell -> Cell Type.\n",
    "\n",
    "There are spaces for you to enter your answers to the questions, either in code or text. Feel free to add more cells if you need (likely).\n",
    "\n",
    "In many parts, some Scikit-learn functions and classes have already been imported to give you leads on what you may need to use. You still need to refer to the Scikit-learn documentation to learn how the classes and methods work. You are free to use other Python data science libraries (NumPy, Pandas, SciPy, Matplotlib/Seaborn, statsmodel, etc). If you are not sure if a library is accepted, please ask.\n",
    "\n",
    "You are expected to turn in a **pdf version** of this notebook with all your **codes, results, and figures** to **GradeScope**. Make sure the figures and results are visible as you want them to appear in the pdf before turning it in. Please do not modify the instructions as doing so will limit our ability to follow and grade your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhdEL6i4VxzH"
   },
   "source": [
    "a) Load the data from '**bean.csv**', which is a dataset for predicting bean shapes. There are 13611 learning examples, 17 features, and 4 classes. The first row contains the names of the columns and the last column contains the labels. Store the data and the labels in variables ***X*** and ***y***, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zEql4zK6EQ9d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import beans csv\n",
    "bean = pd.read_csv(\"~/Desktop/class/bean.csv\")\n",
    "bean.head()\n",
    "\n",
    "# drop last column to make df x\n",
    "x = bean.drop(columns = [\"Class\"]).copy()\n",
    "\n",
    "# select column 17 (class) to make df y\n",
    "y = bean[\"Class\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9K-nXR8WJKQ"
   },
   "source": [
    "Split the data into a training set and a testing set. Use 80% of the data for training and the remaining 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4J98npDmWJ8k"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10888 x_train examples\n",
      "2723 x_test examples\n",
      "10888 y_train examples\n",
      "2723 y_test examples\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test data\n",
    "x_train, x_test = train_test_split(x, test_size=0.2)\n",
    "y_train, y_test = train_test_split(y, test_size=0.2)\n",
    "\n",
    "\n",
    "# check examples for reasonableness\n",
    "print(len(x_train), 'x_train examples')\n",
    "print(len(x_test), 'x_test examples')\n",
    "print(len(y_train), 'y_train examples')\n",
    "print(len(y_test), 'y_test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws-cBleJWZej"
   },
   "source": [
    "Initiate a Decision Tree model from Scikit-Learn. Fit the model on the training set. Print the training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iioupdRHWYkF"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree prediction accuracy training 0.9966936076414401\n"
     ]
    }
   ],
   "source": [
    "# create a single decision tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# train the model / fit onto training set\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "# check training performance\n",
    "predictions = tree.predict(x_train)\n",
    "\n",
    "# evaluate accuracy of training performance\n",
    "currAccuracy = accuracy_score(y_train, predictions)\n",
    "\n",
    "# print training performance\n",
    "print('Decision Tree prediction accuracy training', currAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "428ExXfiWx4j"
   },
   "source": [
    "Evaluate the model on the testing set. Print the testing performance. Is the testing performance the same as the training performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kZveBKFjWtTD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree prediction accuracy testing 0.1652589056188028\n"
     ]
    }
   ],
   "source": [
    "# check testing performance\n",
    "predictions = tree.predict(x_test)\n",
    "\n",
    "# evaluate accuracy of testing performance\n",
    "currAccuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "# print testing performance\n",
    "print('Decision Tree prediction accuracy testing', currAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing performance is not the same as training performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrVr9HVIW89X"
   },
   "source": [
    "Now that have a routine for evaluating our model, is it enough to conclude about our model performance? To be more specific, assume that we have a training set **A** and a testing set **B**. If we train **Decision Tree** model and a **Support Vector Machine** model on **A** and evaluate them on **B**, can we say **Support Vector Machine** is better or worse than **Decision Tree** based on their training performances on **A** or testing performances on **B**? The answer is no because **A** and **B** are just one scenario. We may just have a scenario that is more favorable for **Support Vector Machine** than for **Decision Tree**. To reliably conclude whether a machine learning algorithm is better than another for a specific problem, we need to collect results from multiple scenarios and compare the average performance. Since we only have limited data available to us, we need to use **multi-fold cross-validation** to create multiple **train-validation** scenarios (**Notice**: we use the term **train-validation** instead of **train-test** in **cross-validation** for a reason we will mention in **Problem 3** where we touched model tuning).\n",
    "\n",
    "In **multi-fold cross-validation**, the dataset is splitted into multiple folds of equal size. Each iteration, we pick a fold to be a **validation set** and let the combination of the remaining folds be the **training set**. For example, in **5-fold cross-validation**, we can create **5** different **train-validation** scenarios. Note that for each scenario, you have to reinitiate your model and start training from scratch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijFF9Vpnecln"
   },
   "source": [
    "b) Split the dataset into **5 folds** and store them (for example, a list of lists). Remember to shuffle the data before splitting so that each fold has a reasonable number of examples from different classes. You must do the splitting without any libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w055UcJoffey"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89680499 0.88721528 0.90668626 0.89125643 0.89382807]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# import beans csv\n",
    "bean = pd.read_csv(\"~/Desktop/class/bean.csv\")\n",
    "\n",
    "# shuffling the data\n",
    "bean = bean.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# drop last column to make df x\n",
    "x = bean.drop(columns = [\"Class\"]).copy()\n",
    "\n",
    "# select column 17 (class) to make df y\n",
    "y = bean[\"Class\"]\n",
    "\n",
    "# cross validate and check for reasonableness\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x, y)\n",
    "scores = cross_val_score(dt, x, y, cv=5)\n",
    "print(scores)\n",
    "\n",
    "# split dataset into 5 folds\n",
    "split1 = bean.iloc[0:2722]\n",
    "split2 = bean.iloc[2722:5445]\n",
    "split3 = bean.iloc[5445:8166]\n",
    "split4 = bean.iloc[8166:10888]\n",
    "split5 = bean.iloc[10888:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sRx2cT3fiIC"
   },
   "source": [
    "For each **train-validation** scenario, initiate a new **Decision Tree** model, train it on the **training set** and evaluate it on the **validation set**. Record the training performances and testing performances of all scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 1 training 1.0\n",
      "Decision Tree 1 validation 0.896399706098457\n"
     ]
    }
   ],
   "source": [
    "# set up dataframes\n",
    "scenario1 = bean.iloc[2723:]\n",
    "x_test1 = split1.drop(columns = [\"Class\"]).copy()\n",
    "y_test1 = split1[\"Class\"]\n",
    "x_train1 = scenario1.drop(columns = [\"Class\"]).copy()\n",
    "y_train1 = scenario1[\"Class\"]\n",
    "\n",
    "# create a single decision tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# train the model / fit onto training set\n",
    "tree.fit(x_train1, y_train1)\n",
    "\n",
    "# check performance on training set\n",
    "predictions = tree.predict(x_train1)\n",
    "\n",
    "# evaluate accuracy of training performance\n",
    "trainscore_1 = accuracy_score(y_train1, predictions)\n",
    "\n",
    "# print training performance\n",
    "print('Decision Tree 1 training', trainscore_1)\n",
    "\n",
    "# check performance on validation set\n",
    "predictions = tree.predict(x_test1)\n",
    "\n",
    "# evaluate accuracy of validation performance\n",
    "testscore_1 = accuracy_score(y_test1, predictions)\n",
    "\n",
    "# print validation performance\n",
    "print('Decision Tree 1 validation', testscore_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 2 training 1.0\n",
      "Decision Tree 2 validation 0.8850532500918105\n"
     ]
    }
   ],
   "source": [
    "# scenario 2\n",
    "scenario2 = bean.drop(bean.index[2723:5444])\n",
    "x_test2 = split2.drop(columns = [\"Class\"]).copy()\n",
    "y_test2 = split2[\"Class\"]\n",
    "x_train2 = scenario2.drop(columns = [\"Class\"]).copy()\n",
    "y_train2 = scenario2[\"Class\"]\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(x_train2, y_train2)\n",
    "predictions = tree.predict(x_train2)\n",
    "trainscore_2 = accuracy_score(y_train2, predictions)\n",
    "print('Decision Tree 2 training', trainscore_2)\n",
    "predictions = tree.predict(x_test2)\n",
    "testscore_2 = accuracy_score(y_test2, predictions)\n",
    "print('Decision Tree 2 validation', testscore_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 3 training 1.0\n",
      "Decision Tree 3 validation 0.9040793825799338\n"
     ]
    }
   ],
   "source": [
    "# scenario 3 \n",
    "scenario3 = bean.drop(bean.index[5445:8166])\n",
    "x_test3 = split3.drop(columns = [\"Class\"]).copy()\n",
    "y_test3 = split3[\"Class\"]\n",
    "x_train3 = scenario3.drop(columns = [\"Class\"]).copy()\n",
    "y_train3 = scenario3[\"Class\"]\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(x_train3, y_train3)\n",
    "predictions = tree.predict(x_train3)\n",
    "trainscore_3 = accuracy_score(y_train3, predictions)\n",
    "print('Decision Tree 3 training', trainscore_3)\n",
    "predictions = tree.predict(x_test3)\n",
    "testscore_3 = accuracy_score(y_test3, predictions)\n",
    "print('Decision Tree 3 validation', testscore_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 4 training 1.0\n",
      "Decision Tree 4 validation 0.8901542983100661\n"
     ]
    }
   ],
   "source": [
    "# scenario 4\n",
    "scenario4 = bean.drop(bean.index[8167:10888])\n",
    "x_test4 = split4.drop(columns = [\"Class\"]).copy()\n",
    "y_test4 = split4[\"Class\"]\n",
    "x_train4 = scenario4.drop(columns = [\"Class\"]).copy()\n",
    "y_train4 = scenario4[\"Class\"]\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(x_train4, y_train4)\n",
    "predictions = tree.predict(x_train4)\n",
    "trainscore_4 = accuracy_score(y_train4, predictions)\n",
    "print('Decision Tree 4 training', trainscore_4)\n",
    "predictions = tree.predict(x_test4)\n",
    "testscore_4= accuracy_score(y_test4, predictions)\n",
    "print('Decision Tree 4 validation', testscore_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 5 training 1.0\n",
      "Decision Tree 5 validation 0.8975394785163423\n"
     ]
    }
   ],
   "source": [
    "# scenario 5\n",
    "scenario5 = bean.drop(bean.index[10889:])\n",
    "x_test5 = split5.drop(columns = [\"Class\"]).copy()\n",
    "y_test5 = split5[\"Class\"]\n",
    "x_train5 = scenario5.drop(columns = [\"Class\"]).copy()\n",
    "y_train5 = scenario5[\"Class\"]\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(x_train5, y_train5)\n",
    "predictions = tree.predict(x_train5)\n",
    "trainscore_5 = accuracy_score(y_train5, predictions)\n",
    "print('Decision Tree 5 training', trainscore_5)\n",
    "predictions = tree.predict(x_test5)\n",
    "testscore_5 = accuracy_score(y_test5, predictions)\n",
    "print('Decision Tree 5 validation', testscore_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS0DMKKvgBMK"
   },
   "source": [
    "c) On a same figure, plot the **training performances** and validation performances from different scenarios. Specifically, the x-axis shows the scenarios and the y-axis shows the performance. There should be a line for training performances and a line for validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Gc_LFj0RgnCL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.896399706098457, 0.8850532500918105, 0.9040793825799338, 0.8901542983100661, 0.8975394785163423]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [trainscore_1, trainscore_2, trainscore_3, trainscore_4, trainscore_5]\n",
    "y = [testscore_1, testscore_2, testscore_3, testscore_4, testscore_5]\n",
    "print(y)\n",
    "plt.plot(x, label = \"training performance\")\n",
    "plt.plot(y, label = \"testing performance\")\n",
    "leg = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vntnt1KYhNIQ"
   },
   "source": [
    "How do the performances differ across different train-validation splits? How are the validation performances compared to the training performances in general? What may be the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training performances usually return a higher accuracy score while the validation performances are lower. While I was creating the different train-validation scenarios, I noticed that all of my training models were returning a 1.0 accuracy score. This could perhaps be due to overfitting, which then denigrates the accuracy of the validation model. Overfitting of the model could have also led to the differences in performances across the different train-validation splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXaOUZe7ERWo"
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "**Cross-validation** is useful for comparing different algorithms. In a similar way, **cross-validation** can also be used to tune your learning model. If you take a look at the **Scikit-Learn** documentation of **Decision Tree**, you can see there are many parameters that we can specify when initiating a model. These are called **hyper-parameters** because they are not learned by the model but control how the model learns. You can think of the model as a learning machine and the **hyper-parameters** as knobs to configure the machine. Depending on how you choose the values of these **hyper-parameters**, the resulting model may have varying performances. We can use **cross-validation** to compared between different models of the same learning algorithm but with different **hyper-parameter settings**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ya7K2NUZjnry"
   },
   "source": [
    "Which values should we specify for the **hyper-parameters** of our learning algorithm? That depends on the task. As data scientists, we need to understand the mearning of each **hyper-parameter** and the effect it may have on the learning process. For this problem, you are free to consult any learning resource but you need to answer in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOioPy2Ak3sI"
   },
   "source": [
    "a) Explain the meanings and effects of the following hyper-parameters of **Decision Tree**. The effects mean what happens when you vary the value of a hyper-parameter:\n",
    "\n",
    "\n",
    "Maximum depth:\n",
    "The depth of a decision tree is measured by how many splits the tree makes before coming to a decision or prediction. The maximum depth of a decison tree, therefore, is the longest pathway from the root of the tree (the node starting the graph) to the leaf of the tree (the final decision where the branch doesn't split anymore). On the way up from the root to the leaf, the model goes through different decision nodes, which are essentially questions or places where the tree splits. The deeper that a tree is (meaning, the more decisions or splits it has), means that it has a huge number of splits. A huge number of splits means that the tree may be too complex, and it could lead to overfitting. When you vary the value of a tree's maximum depth, you need to choose a depth that avoids underfitting or overfitting the model. \n",
    "\n",
    "Split criterion:\n",
    "The split criterion for a decision tree is the criteria that we use to split the data for testing and training. In other words, we need to consider what's important to us when we split our data into a training set and a testing set. For example, we could have 3 classes: kiwis, apples, and mangoes. We can split the classes by the criteria that it needs to split into kiwis for testing and apples/mangoes for training, apples for testing and kiwis/mangoes for trianing, and mangoes for testing and kiwis/apples for training. There are a lot of different ways that we can decide on our split criterion, such as considering every twentieth percentile (20%, 40%, 60%, 80%, 100%), or other methods. We want to have split criterion in order to compare split points and choose the best hyper-parameters for our decision tree pathway. If there are too few splits, the tree will be underfit and if there are too many splits, the tree will be overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kKEiYvclf9N"
   },
   "source": [
    "b) Explain the meanings and effects of the following hyper-parameters of **Random Forest**. The effects mean what happens when you vary the value of a hyper-parameter:\n",
    "\n",
    "\n",
    "Number of base learners/ estimators:\n",
    "A base learner / estimator in a random forest is each decision tree in a random forest. Each base learner or estimator could yield poor estimates. If we have a random forest, we want to have a huge number of trees, especially if we have weak estimators. Due to the weak law of large numbers, we can see that the ensemble (the random forest) will tend towards a prediction as the number of trees (base learners / estimators) tends towards infinity. Therefore, we want to have a LOT of trees. \n",
    "\n",
    "Maximum number of test features:\n",
    "\"Features\" are some sort of property of our data, for example names, ages, age, etc. They're usually the column names. When we create a random forest, it looks for the best features to use among some random subset of features. This is usually better than just looking for a feaure while we're splitting the node in each individual tree because it introduces a lot more diversity into the model. We can measure the importance of each feature to see which ones we should drop (if they don't contribute too little or nothing at all to the prediction process). If we select a lot of features, then we can increase the strength of each individual tree, while if we select a low number of features, we increase the strength of the forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ite2YRYcET_P"
   },
   "source": [
    "## Problem 3\n",
    "\n",
    "In this problem, we will implement hyper-parameter tuning for both Decision Tree and Random Forest. \n",
    "\n",
    "Remember we distinguished between **test** and **validation** in Problem 1? Here is the reason. Typically, while tuning the hyperparameters, the model is not allowed to see the testing set. If we use the performance of the model on the testing set to guide the hyper-parameter tuning, our model's configuration will be biased towards the examples in the testing set. As a result, our model's testing performance is no longer a proxy for its performance on future unseen data. As a rule of thumb, no test data is allowed during training and tuning a machine learning model.\n",
    "\n",
    "To guide our model through hyper-parameter tuning, we need to further reserve a portion of our data as the **validation set**. In total, we split our data into 3 sets: a **training set**, a **validation set**, and a **test set**. Our model will be trained on the **training set** and evaluated on the **validation set**. Based on the model's **validation performance**, we make changes to the values of its **hyper-parameters**, a.k.a **tuning**, and repeat the train-validation process. Once we are happy with the hyper-parameters of our model, we can now test it on the testing set and use the result as a proxy for future performances.\n",
    "\n",
    "We can do even better. Instead of only one train-validation scenario, we can create multiple train-validation scenarios via **cross-validation**. The **average performance** of the model on the scenarios is used to guide the tuning process.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src='https://scikit-learn.org/stable/_images/grid_search_cross_validation.png' width=600 height=400>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "\n",
    "**Hyper-parameter tuning** consists of cross-validation nested within an outer loop that iterates through a list of hyper-parameter combinations. For example, if each hyper-parameter set consists of 2 hyper-parameters **Maximum depth** and **Split criterion**. If we want to check **Maximum depths** in [1,2,3,4,5] and **Split criteria** in [\"gini\",\"entropy\"] then there are **2 x 5 = 10 hyper-parameter combinations**. For each combination, we initiate a model with the corresponding hyper-parameters and record the cross-validation average performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WL8csliIjpRs"
   },
   "source": [
    "First, let's again import our data from '**bean.csv**. Split the data into a **training set** (80%) and a **testing set** (20%). We will do **hyper-parameter tuning** via **cross-validation** on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6coD09jtkIV0"
   },
   "outputs": [],
   "source": [
    "# import beans csv\n",
    "bean = pd.read_csv(\"~/Desktop/class/bean.csv\")\n",
    "\n",
    "# shuffling the data\n",
    "bean = bean.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split data into training/validation set and testing set\n",
    "bean_train = bean.iloc[:10888]\n",
    "bean_test = bean.iloc[10888:]\n",
    "\n",
    "# drop last column to make df x\n",
    "x = bean_train.drop(columns = [\"Class\"]).copy()\n",
    "\n",
    "# select column 17 (class) to make df y\n",
    "y = bean_train[\"Class\"]\n",
    "\n",
    "x_test = bean_test.drop(columns = [\"Class\"]).copy()\n",
    "y_test = bean_test[\"Class\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbMObdLmtq16"
   },
   "source": [
    "a) Let us try a simple tuning for the hyper-parameter Maximum depth of Decision Tree. The values of the depth are in [2,3,5,8,15]. For the cross-validation step, you can use the cross_validate function from Scikit-Learn. Record the average training performance and validating performance for each value of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JoOuOZjVEYJU"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 8, 15] [0.6577195045234255, 0.7750964093724748, 0.8904986463530216, 0.9403012620726713, 0.9932494315157168]\n",
      "[2, 3, 5, 8, 15] [0.6547576234217567, 0.7714915893810954, 0.8798679575645375, 0.8987871364077152, 0.8946543144730809]\n"
     ]
    }
   ],
   "source": [
    "depths = [2,3,5,8,15]\n",
    "depth_train_scores = []\n",
    "depth_valid_scores = []\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth = depth)\n",
    "    scores = cross_validate(estimator=tree, X=x, y=y, cv=5, return_train_score = True)\n",
    "    depth_train_score = np.mean(scores['train_score'])\n",
    "    depth_valid_score = np.mean(scores['test_score'])\n",
    "    depth_train_scores.append(depth_train_score)\n",
    "    depth_valid_scores.append(depth_valid_score)\n",
    "    \n",
    "print(depths, depth_train_scores)\n",
    "print(depths, depth_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njyrTjG7u6yh"
   },
   "source": [
    "Plot the **training performances** and **testing performances** versus the **maximum depths**. What are the general trends. What may be the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hNgIJ6UvvJ91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.63, 1.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = [2,3,5,8,15]\n",
    "plt.plot(y, depth_train_scores, label = \"training performance\")\n",
    "plt.plot(y, depth_valid_scores, label = \"validation performance\")\n",
    "leg = plt.legend(loc='upper right')\n",
    "plt.ylim([0.63,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Past a certain point, as the maximum depth of the tree increases, the training performance accuracy score increases wihle the validation performance accuracy score decreases. This may be because a deeper tree means a more complex model, which leads to overfitting of the training model. This overfitting thus leads to a sharp increase in training performance accuracy accompanied by a simultaneous drop in testing performance accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "th6t87aUjMfG"
   },
   "source": [
    "Finally, test the best model on the testing set and print out the performance. How is the testing performance compared to the cross-validation performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "i0JC46DSjZgL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree depth 5 testing performance 0.8868894601542416\n"
     ]
    }
   ],
   "source": [
    "# depth of 5\n",
    "x = bean_train.drop(columns = [\"Class\"]).copy()\n",
    "y = bean_train[\"Class\"]\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "tree.fit(x,y)\n",
    "predictions = tree.predict(x_test)\n",
    "test_score = accuracy_score(y_test, predictions)\n",
    "print('Decision Tree depth 5 testing performance', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing performance is not as good as the training performance (as expected), but it returns approximately the same accuracy score as the cross-validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzxfpiYFvMna"
   },
   "source": [
    "b) Do the same tuning for the hyper-parameter **Number of base learners** of **Random Forest**. The values are in [5,10,15,20,25,30,35,40,45,50]. Record the average training performance and testing performance for each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "blxKNWrUvL2C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 10, 15, 20, 25, 30, 35, 40, 45, 50] [0.9872336216359985, 0.9941219680673485, 0.9971528351197222, 0.9980253532194208, 0.998920833431634, 0.9995178193611123, 0.9995637488581218, 0.9996326246517034, 0.9997015004452848, 0.9998852026173803]\n",
      "[5, 10, 15, 20, 25, 30, 35, 40, 45, 50] [0.9088909304343389, 0.9142173815661099, 0.9155951716606495, 0.9175237572197525, 0.9189020534825856, 0.9204627812344853, 0.9205542711535111, 0.9210138297831956, 0.9201872569601305, 0.9201870038759837]\n"
     ]
    }
   ],
   "source": [
    "number_of_base_learners = [5,10,15,20,25,30,35,40,45,50]\n",
    "base_train_scores = []\n",
    "base_valid_scores = []\n",
    "for base in number_of_base_learners:\n",
    "    tree = RandomForestClassifier(n_estimators = base)\n",
    "    scores = cross_validate(estimator=tree, X=x, y=y, cv=5, return_train_score = True)\n",
    "    base_train_score = np.mean(scores['train_score'])\n",
    "    base_valid_score = np.mean(scores['test_score'])\n",
    "    base_train_scores.append(base_train_score)\n",
    "    base_valid_scores.append(base_valid_score)\n",
    "    \n",
    "print(number_of_base_learners, base_train_scores)\n",
    "print(number_of_base_learners, base_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7izHxidvsW-"
   },
   "source": [
    "Plot the **training performances** and **testing performances** versus the **number of base learners**. What are the general trends. What may be the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JyWl1zSXvvV5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_base_learners = [5,10,15,20,25,30,35,40,45,50]\n",
    "plt.plot(number_of_base_learners, base_train_scores, label = \"training performance\")\n",
    "plt.plot(number_of_base_learners, base_valid_scores, label = \"validation performance\")\n",
    "leg = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, as the number of base learners increases, the training and validation performances increase as well. With more trees comes more diversity, as each individual tree learns from a random sample and splits at each node based on a random set of features. As I mentioned prevoiusly (refer to problem 2B), due to the weak law of large numbers, we want a bigger number of base learners, especially if they are weak estimators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8xST_qojc3f"
   },
   "source": [
    "Finally, test the best model on the testing set and print out the performance. How is the testing performance compared to the cross-validation performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jok10aGMjeO8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest 50 Base Learners testing performance 0.927653323540213\n"
     ]
    }
   ],
   "source": [
    "# 50 base learners\n",
    "tree = RandomForestClassifier(n_estimators = 50)\n",
    "tree.fit(x,y)\n",
    "predictions = tree.predict(x_test)\n",
    "test_score = accuracy_score(y_test, predictions)\n",
    "print('Random Forest 50 Base Learners testing performance', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing performance yields a lower accuracy score than the training performance, but an approximately equivalent accuracy score to the cross-validation performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ncaNdNuTtRQ"
   },
   "source": [
    "## Problem 4\n",
    "\n",
    "**This is an open-ended problem**\n",
    "\n",
    "We are going to train a binary classifier for a set of red points and another binary classifier for a set of blue points. \n",
    "\n",
    "You are given a red dataset in '**R.csv**' and a blue dataset in '**B.csv**'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDa9zGGrUkYb"
   },
   "source": [
    "a) Load the datasets. All datasets are 2-D with the last column containing the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "J61x9AztUky1"
   },
   "outputs": [],
   "source": [
    "red = pd.read_csv(\"~/Desktop/class/R.csv\")\n",
    "blue = pd.read_csv(\"~/Desktop/class/B.csv\")\n",
    "red_x = red.drop(columns = \"0\")\n",
    "red_y = red[\"0\"]\n",
    "blue_x = blue.drop(columns = \"1\")\n",
    "blue_y = blue[\"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyWq8FdKUlfA"
   },
   "source": [
    "b) Train 2 classifiers, one for the red dataset and the other for the blue dataset. You are free to choose the learning algorithm from Scikit-Learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "dkPZEyu8UtJn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red Accuracy: 0.96875\n",
      "Blue Accuracy: 0.9375\n",
      "Red Coefficient [[0.05126917 1.76644738]]\n",
      "Blue Coefficient [[0.00389515 1.51133017]]\n",
      "Red Intercept [-1.40695099]\n",
      "Blue Intercept [-0.13824949]\n"
     ]
    }
   ],
   "source": [
    "# train red classifier using svm\n",
    "red_x_train, red_x_test, red_y_train, red_y_test = train_test_split(red_x, red_y, test_size = 0.2)\n",
    "red_classifier = svm.SVC(kernel = 'linear')\n",
    "red_classifier.fit(red_x_train, red_y_train)\n",
    "red_y_predict = red_classifier.predict(red_x_test)\n",
    "print(\"Red Accuracy:\", metrics.accuracy_score(red_y_test, red_y_predict))\n",
    "\n",
    "# train blue classifier using svm\n",
    "blue_x_train, blue_x_test, blue_y_train, blue_y_test = train_test_split(blue_x, blue_y, test_size = 0.2)\n",
    "blue_classifier = svm.SVC(kernel = 'linear')\n",
    "blue_classifier.fit(blue_x_train, blue_y_train)\n",
    "blue_y_predict = blue_classifier.predict(blue_x_test)\n",
    "print(\"Blue Accuracy:\", metrics.accuracy_score(blue_y_test, blue_y_predict))\n",
    "print(\"Red Coefficient\", red_classifier.coef_)\n",
    "print(\"Blue Coefficient\", blue_classifier.coef_)\n",
    "print(\"Red Intercept\", red_classifier.intercept_)\n",
    "print(\"Blue Intercept\", blue_classifier.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRZFPGBzC1TI"
   },
   "source": [
    "What are the **decision boundaries** of the classifiers? Are they the same? What does it tell you about the difference between the **R** population and the **B** population. The **decision boundary** is the values that separate the positive instances from the negative instances. For example, if a classifier decides that all instances with feature value > 5 are positive and all instances with feature value < 5 are negative, then 5 is the decision boundary of that classifier. You can answer by showing/contrasting the decision boundaries on a figure/figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary for the R population is the plane satisfying (0.05126917 1.76644738)*x - 1.40695099 = 0.\n",
    "The decision boundary for the B population is the plane satisfying (0.00389515 1.51133017)*x  - 0.13824949 = 0.\n",
    "*asterisk symbol refers to dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKwJ_xImUxmS"
   },
   "source": [
    "c) Now, imagine that we combine the **R** population and the **B** population into one single population. Given the classifiers that we trained from the previous part, how should the joint classifier be chosen so that it is fair to both groups? How would you express the fairness constraint explicitly? (**Note**: Don't worry about right or wrong here)\n",
    "\n",
    "As a motivation, consider a real example in which you work for a financial company that accepts loan application. You have data of people from two different areas of your city, each with a different social-economic situation. Normally, our company has internal processes to deal with people from either areas (2 classifiers where positive means granting loan and negative means no loan). However, due to COVID-19, we are not in a good financial state right now so we can only grant loans to a limited number of people from either areas. If we only care about profit, we will just give loans to those in the better area, but is it socially fair to those from the other area? How should we adjust/combine our internal processes (the classifiers) to process the applications in a fair way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c6WbHEEUw4e"
   },
   "source": [
    "Firstly, I would introduce a categorical variable that distinguishes between the R and B populations. Then, I either manually change the decision boundaries after fitting the model to ensure that the model is fairly distributed, or I can adjust the cost function (the measure of error) if the distribution of assignments leads to unfair outputs. I would do this by calculating the \"unfairness\" and adjust the cost function accordingly."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fall21_HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
